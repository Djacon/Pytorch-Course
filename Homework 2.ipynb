{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1PkQAjVNo5ZLZcWqSvJ1cEiwCKGi1rPim","timestamp":1670257978288},{"file_id":"1EU3DbXL-rWn4_oN1Uo9uW4d1a-vATjkc","timestamp":1662646792259}],"collapsed_sections":["4PNGWOK6UpCE","5bbP969nVEqz","1p1c7n8VVcwk","awJayvOSSYpB","HMo54dppWSX2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Занятие 2.**"],"metadata":{"id":"zjM7DFps2rBi"}},{"cell_type":"markdown","source":["### [Домашнее задание в Colab](https://colab.research.google.com/drive/1sEI3kxp9OvGztDkA-Qo0ALxyMldd1EA_#scrollTo=qXc4AbAMDhO8)"],"metadata":{"id":"qXc4AbAMDhO8"}},{"cell_type":"markdown","source":["# [Pytorch autograd](https://pytorch.org/docs/stable/autograd.html)"],"metadata":{"id":"KlS1ciOPBg7g"}},{"cell_type":"markdown","source":["[Tutorial](https://www.youtube.com/watch?v=MswxJw-8PvE)\n","\n","[Slides](https://app.diagrams.net/#G1bq3akhmA5DGRCiFYJfNPSn7il2wvCkEY)\n","\n","[Torch C++ Binary operations](https://github.com/pytorch/pytorch/blob/c5872e6d6d8fd9b8439b914c143d49488335f573/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp)\n","\n","[Torch C++ Activations](https://github.com/pytorch/pytorch/blob/c5872e6d6d8fd9b8439b914c143d49488335f573/aten/src/ATen/native/cpu/Activation.cpp)"],"metadata":{"id":"kP06X1SrzLlm"}},{"cell_type":"markdown","source":["# Создание собственной библиотеки автоматического дифференцирования"],"metadata":{"id":"B8urvcsAKi62"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"sPyPkdq5RH94"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Практическое задание: написать собственный движок автоматического дифференцирования, а именно: реализовать"],"metadata":{"id":"fA2PNhudUNij"}},{"cell_type":"markdown","source":["### Создание собственного класса Function для реализации кастомных функций"],"metadata":{"id":"4PNGWOK6UpCE"}},{"cell_type":"code","source":["class Function:\n","    '''Simple Function class for creating custom activation functions'''\n","    def __init__(self, func, deriv):\n","        self.func = func    # Custom function\n","        self.deriv = deriv  # Its derivative\n","\n","    def __call__(self, input): # Calculate function\n","        out = Value(self.func(input.data), _children=(input,))\n","\n","        def _backward():\n","            input.grad += out.grad * self.deriv(input.data)\n","        out._backward = _backward\n","        out.grad_fn = f'<{self.__class__.__name__}Backward>'\n","        return out\n","\n","    def __repr__(self):\n","        return f'{self.__class__.__name__}'"],"metadata":{"id":"G2FjGULZH1As"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Создание базовых функций через Function"],"metadata":{"id":"5bbP969nVEqz"}},{"cell_type":"code","source":["from math import exp\n","\n","class Exp(Function):\n","    def __init__(self):\n","        super().__init__(exp, exp)\n","\n","class ReLU(Function):\n","    def __init__(self):\n","        func = lambda input: max(input, 0)\n","        deriv = lambda input: 1 if input > 0 else 0\n","        super().__init__(func, deriv)\n","\n","class LeakyReLU(Function):\n","    def __init__(self, slope=0.01):\n","        func = lambda input: max(input, input * slope)\n","        deriv = lambda input: 1 if input > 0 else slope\n","        super().__init__(func, deriv)\n","\n","class Sigmoid(Function):\n","    def __init__(self):\n","        func = lambda input: 1 / (1 + exp(-input))\n","        deriv = lambda input: func(input) * (1 - func(input))\n","        super().__init__(func, deriv)\n","\n","class Tanh(Function):\n","    def __init__(self):\n","        def func(input):\n","            exp1, exp2 = exp(input), exp(-input)\n","            return (exp1 - exp2) / (exp1 + exp2)\n","        deriv = lambda input: 1 - func(input) ** 2\n","        super().__init__(func, deriv)\n","\n","Exp = Exp()\n","ReLU = ReLU()\n","LeakyReLU = LeakyReLU(0.01)\n","Sigmoid = Sigmoid()\n","Tanh = Tanh()"],"metadata":{"id":"JntlSkAMU-6n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Создадим класс MultiFunction для реализации простых арифметических операций с двумя переменными"],"metadata":{"id":"1p1c7n8VVcwk"}},{"cell_type":"code","source":["class MultiFunction:\n","    def __init__(self, func, deriv1, deriv2):\n","        self.func = func\n","        self.deriv1 = deriv1\n","        self.deriv2 = deriv2\n","\n","    def __call__(self, value, other):\n","      if isinstance(other, Value):\n","          out = Value(self.func(value.data, other.data), _children=(value, other))\n","          def _backward():\n","              value.grad += out.grad * self.deriv1(value.data, other.data)\n","              other.grad += out.grad * self.deriv2(value.data, other.data)\n","      else:\n","          # Ignore `other` value if it's a constant (no need gradient for it)\n","          out = Value(self.func(value.data, other), _children=(value,))\n","          def _backward():\n","              value.grad += out.grad * self.deriv1(value.data, other)\n","\n","      out._backward = _backward\n","      out.grad_fn = f'<{self.__class__.__name__}Backward>'\n","      return out"],"metadata":{"id":"7s8RVNfG96Ne"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from math import log\n","\n","class Add(MultiFunction):\n","    def __init__(self):\n","        func = lambda a, b: a + b\n","        deriv = lambda a, b: 1\n","        super().__init__(func, deriv, deriv)\n","\n","class Mul(MultiFunction):\n","    def __init__(self):\n","        func = lambda a, b: a * b\n","        deriv1 = lambda a, b: b\n","        deriv2 = lambda a, b: a\n","        super().__init__(func, deriv1, deriv2)\n","\n","class Div(MultiFunction):\n","    def __init__(self):\n","        func = lambda a, b: a / b\n","        deriv1 = lambda a, b: 1 / b\n","        deriv2 = lambda a, b: -a / (b * b)\n","        super().__init__(func, deriv1, deriv2)\n","\n","class Pow(MultiFunction):\n","    def __init__(self):\n","        func = lambda a, b: a ** b\n","        deriv1 = lambda a, b: b * a ** (b - 1)\n","        deriv2 = lambda a, b: a ** b * log(a)\n","        super().__init__(func, deriv1, deriv2)\n","\n","Add = Add()\n","Mul = Mul()\n","Div = Div()\n","Pow = Pow()"],"metadata":{"id":"-HU3ewHSTJOq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Реализация класса Value для автоматического дифференцирования"],"metadata":{"id":"occIQU7NVgra"}},{"cell_type":"code","source":["class Value:\n","    \"\"\" Stores a single scalar value and its gradient \"\"\"\n","\n","    def __init__(self, data, _children=()):\n","        self.data = data\n","        self.grad = 0\n","        # Internal variables used for autograd graph construction\n","        self._backward = lambda: None # Backward function \n","        self._prev = set(_children)   # Set of Value objects\n","        self._childs = None           # Full list of Values (need for Loss)\n","        self.grad_fn = None           # Name of the gradient function used\n","\n","    def relu(self):\n","        return ReLU(self)\n","\n","    def exp(self):\n","        return Exp(self)\n","\n","    def __add__(self, other):\n","        return Add(self, other)\n","\n","    def __mul__(self, other):\n","        return Mul(self, other)\n","\n","    def __pow__(self, other):\n","        return Pow(self, other)\n","\n","    def get_childs(self):\n","        if self._childs:\n","            return self._childs\n","        topo = []\n","        visited = set()\n","        def build_topo(v):\n","            if v not in visited:\n","                visited.add(v)\n","                for child in v._prev:\n","                    build_topo(child)\n","                topo.append(v)\n","        build_topo(self)\n","        self._childs = topo[::-1]\n","        return self._childs\n","\n","    def backward(self):\n","        # topological order all of the children in the graph\n","        topo = self.get_childs()\n","        # go one variable at a time and apply the chain rule to get its gradient\n","        self.grad = 1\n","        for v in topo:\n","            v._backward()\n","\n","    def zero_grad(self):\n","        topo = self.get_childs()\n","        for v in topo:\n","            v.grad = 0\n","    \n","    def update(self, lr=0.01):\n","        topo = self.get_childs()\n","        for v in topo[1:]:\n","            v.data -= lr * v.grad\n","\n","    def __neg__(self): # -self\n","        return self * -1\n","\n","    def __radd__(self, other): # other + self\n","        return self + other\n","\n","    def __sub__(self, other): # self - other\n","        return self + (-other)\n","\n","    def __rsub__(self, other): # other - self\n","        return other + (-self)\n","\n","    def __rmul__(self, other): # other * self\n","        return Mul(self, other)\n","\n","    def __truediv__(self, other): # self / other\n","        return Div(self, other)\n","\n","    def __rtruediv__(self, other): # other / self\n","        return other * self ** -1\n","\n","    def __repr__(self):\n","        grad_fn = f', grad_fn={self.grad_fn}' if self.grad_fn else ''\n","        return f\"Value(data={round(self.data, 4)}, grad={round(self.grad, 4)}{grad_fn})\""],"metadata":{"id":"IgigjU-3SUnq","executionInfo":{"status":"ok","timestamp":1670676295893,"user_tz":-180,"elapsed":3,"user":{"displayName":"Djacon","userId":"03384204586246157387"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### Сравниваем работу нашей библиотеки с Pytorch"],"metadata":{"id":"awJayvOSSYpB"}},{"cell_type":"code","source":["a = Value(-4.9)\n","\n","b = ReLU(a) # or a.relu()\n","c = LeakyReLU(a)\n","d = Sigmoid(c)\n","e = Tanh(d)\n","\n","y = b * c + d\n","y.backward()\n","\n","print(a, b, c, d, e, y, sep='\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4i3d2suhdL2a","executionInfo":{"status":"ok","timestamp":1670676320131,"user_tz":-180,"elapsed":505,"user":{"displayName":"Djacon","userId":"03384204586246157387"}},"outputId":"e757b6d8-86dc-4039-a31c-45df694659d2"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Value(data=-4.9, grad=0.0025)\n","Value(data=0, grad=-0.049, grad_fn=<ReLUBackward>)\n","Value(data=-0.049, grad=0.2498, grad_fn=<LeakyReLUBackward>)\n","Value(data=0.4878, grad=1, grad_fn=<SigmoidBackward>)\n","Value(data=0.4524, grad=0, grad_fn=<TanhBackward>)\n","Value(data=0.4878, grad=1, grad_fn=<AddBackward>)\n"]}]},{"cell_type":"code","source":["a = torch.tensor(-4.9, requires_grad=True)\n","\n","b = a.relu()\n","c = torch.nn.LeakyReLU()(a)\n","d = c.sigmoid()\n","e = d.tanh()\n","\n","y = b * c + d\n","y.backward()\n","\n","print(a, b, c, d, e, y, sep='\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GAKQvd1egH-a","executionInfo":{"status":"ok","timestamp":1670676322931,"user_tz":-180,"elapsed":458,"user":{"displayName":"Djacon","userId":"03384204586246157387"}},"outputId":"53bb6b1b-b0fa-4a15-b412-f57104b0f36c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(-4.9000, requires_grad=True)\n","tensor(0., grad_fn=<ReluBackward0>)\n","tensor(-0.0490, grad_fn=<LeakyReluBackward0>)\n","tensor(0.4878, grad_fn=<SigmoidBackward0>)\n","tensor(0.4524, grad_fn=<TanhBackward0>)\n","tensor(0.4878, grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"markdown","source":["### Рассмотрим более сложные тесты"],"metadata":{"id":"HMo54dppWSX2"}},{"cell_type":"code","source":["def test_sanity_check():\n","\n","    x = Value(-4.0)\n","    z = 2 * x + 2 + x\n","  \n","    q = z.relu() + z * x\n","    h = (z * z).relu()\n","    y = h + q + q * x\n","    y.backward()\n","    xmg, ymg = x, y\n","\n","    x = torch.Tensor([-4.0]).double()\n","    x.requires_grad = True\n","    z = 2 * x + 2 + x\n","    q = z.relu() + z * x\n","    h = (z * z).relu()\n","    y = h + q + q * x\n","    y.backward()\n","    xpt, ypt = x, y\n","\n","    \n","    # forward pass went well\n","    assert ymg.data == ypt.data.item()\n","    # backward pass went well\n","    # print(xmg, xpt, xpt.grad)\n","    assert xmg.grad == xpt.grad.item()\n","    print('Test passed!')\n","\n","\n","def test_more_ops():\n","\n","    a = Value(-4.0)\n","    b = Value(2.0)\n","    c = a + b\n","    d = a * b + b**3\n","    c += c + 1\n","    c += 1 + c + (-a)\n","    d += d * 2 + (b + a).relu()\n","    d += 3 * d + (b - a).relu()\n","    e = c - d\n","    f = e**2\n","    g = f / 2.0\n","    g += 10.0 / f\n","    g.backward()\n","    amg, bmg, gmg = a, b, g\n","\n","    a = torch.Tensor([-4.0]).double()\n","    b = torch.Tensor([2.0]).double()\n","    a.requires_grad = True\n","    b.requires_grad = True\n","    c = a + b\n","    d = a * b + b**3\n","    c = c + c + 1\n","    c = c + 1 + c + (-a)\n","    d = d + d * 2 + (b + a).relu()\n","    d = d + 3 * d + (b - a).relu()\n","    e = c - d\n","    f = e**2\n","    g = f / 2.0\n","    g = g + 10.0 / f\n","    g.backward()\n","    apt, bpt, gpt = a, b, g\n","\n","    tol = 1e-6\n","    # forward pass went well\n","    assert abs(gmg.data - gpt.data.item()) < tol\n","    # backward pass went well\n","    # print(amg, apt.data, apt.grad)\n","    # print(bmg, bpt.data, bpt.grad)\n","    assert abs(amg.grad - apt.grad.item()) < tol\n","    assert abs(bmg.grad - bpt.grad.item()) < tol\n","    print('Test passed!')"],"metadata":{"id":"vY7OzWjuUiaa","executionInfo":{"status":"ok","timestamp":1670676348527,"user_tz":-180,"elapsed":216,"user":{"displayName":"Djacon","userId":"03384204586246157387"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["test_sanity_check()"],"metadata":{"id":"w9n8DN6RYkrx","executionInfo":{"status":"ok","timestamp":1670676348527,"user_tz":-180,"elapsed":2,"user":{"displayName":"Djacon","userId":"03384204586246157387"}},"outputId":"0bfde7d7-ba93-4e11-c998-b93532cf558e","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Test passed!\n"]}]},{"cell_type":"code","source":["test_more_ops()"],"metadata":{"id":"1T198QDQYh_q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670676348527,"user_tz":-180,"elapsed":2,"user":{"displayName":"Djacon","userId":"03384204586246157387"}},"outputId":"9433b198-f090-4500-93d6-373a8aed8138"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Test passed!\n"]}]},{"cell_type":"markdown","source":["# Обучение на основе собственной библиотеки"],"metadata":{"id":"o-KbDOhMYHZ1"}},{"cell_type":"markdown","source":["### Многослойный перцептрон на основе класса Value"],"metadata":{"id":"uVK1JLXom0Ze"}},{"cell_type":"code","source":["import random\n","\n","class Neuron:\n","    def __init__(self, nin, activation=None):\n","        self.w = [Value(random.uniform(0, 1)) for _ in range(nin)]\n","        self.b = Value(random.uniform(0, 1))\n","        self.nin = nin\n","        self.activation = activation\n","\n","    def __call__(self, xs):\n","        out = self.b.data\n","        for i in range(self.nin):\n","            out += self.w[i].data * xs[i].data\n","        out = Value(out, _children=tuple([*self.w, *xs, self.b]))\n","\n","        def _backward():\n","            for i in range(self.nin):\n","                self.w[i].grad += out.grad * xs[i].data\n","                xs[i].grad += out.grad * self.w[i].data\n","            self.b.grad += out.grad\n","\n","        out._backward = _backward\n","        out.grad_fn = '<LinearBackward>'\n","        return self.activation(out) if self.activation else out\n","\n","    def params(self):\n","        return (self.w, self.b)\n","\n","    def __repr__(self):\n","        act = f'{self.activation}' if self.activation else ''\n","        return f'{act}Neuron({self.nin})'\n","\n","class Layer:\n","    def __init__(self, nin, nout, activation=None):\n","        self.neurons = [Neuron(nin, activation) for _ in range(nout)]\n","        self.nin = nin\n","        self.nout = nout\n","        self.activation = activation\n","\n","    def __call__(self, x):\n","        out = [neuron(x) for neuron in self.neurons]\n","        return out\n","\n","    def params(self):\n","        return [neuron for neuron in self.neurons]\n","\n","    def __repr__(self):\n","        act = f', activation=<{self.activation}>' if self.activation else ''\n","        return f'Linear({self.nin}, {self.nout}{act})'"],"metadata":{"id":"rkl70dxhkcQN","executionInfo":{"status":"ok","timestamp":1670676393356,"user_tz":-180,"elapsed":219,"user":{"displayName":"Djacon","userId":"03384204586246157387"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class Sequential:\n","    def __init__(self, *layers):\n","        self.layers = list(layers)\n","\n","    def __call__(self, xs):\n","        out = [x if isinstance(x, Value) else Value(x) for x in xs]\n","        for layer in self.layers:\n","            out = layer(out)\n","        return out[0] if len(out) == 1 else out\n","\n","    def params(self):\n","        return [layer for layer in self.layers]\n","      \n","    def add(self, layer):\n","        self.layers.append(layer)\n","\n","    def __repr__(self):\n","        repr = ''.join(f'  {layer}\\n' for layer in self.layers)\n","        return f'Sequential(\\n{repr})'"],"metadata":{"id":"XnF4O_5oYDlC","executionInfo":{"status":"ok","timestamp":1670676395554,"user_tz":-180,"elapsed":2,"user":{"displayName":"Djacon","userId":"03384204586246157387"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["## Обучение многослойного перцептрона"],"metadata":{"id":"YkkaE1V1m5i5"}},{"cell_type":"markdown","source":["Сам перцептрон"],"metadata":{"id":"WWy-H8eCn2zm"}},{"cell_type":"code","source":["model = Sequential()\n","model.add(Layer(3, 5, Sigmoid))\n","model.add(Layer(5, 5, ReLU))\n","model.add(Layer(5, 1))\n","print(model)\n","print(\"Number of parameters\", len(model.params()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rhy71qE0YtmS","executionInfo":{"status":"ok","timestamp":1670676397284,"user_tz":-180,"elapsed":274,"user":{"displayName":"Djacon","userId":"03384204586246157387"}},"outputId":"65322c2d-b8dd-445f-92e5-5ce085b40d1e"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequential(\n","  Linear(3, 5, activation=<Sigmoid>)\n","  Linear(5, 5, activation=<ReLU>)\n","  Linear(5, 1)\n",")\n","Number of parameters 3\n"]}]},{"cell_type":"markdown","source":["Набор данных"],"metadata":{"id":"OvkZVOLcnvqu"}},{"cell_type":"code","source":["xs = [\n","  [2.,  3., -1.],\n","  [3., -1.,  .5],\n","  [.5,  1.,  1.],\n","  [1.,  1., -1.],\n","]\n","ys = [1.0, -1.0, -1.0, 1.0] # desired targets (y = -0.148x1 + 0.074x2 - 1.037x3 + 0.037)"],"metadata":{"id":"aLJULsNanpVC","executionInfo":{"status":"ok","timestamp":1670676400457,"user_tz":-180,"elapsed":228,"user":{"displayName":"Djacon","userId":"03384204586246157387"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["%%time\n","for k in range(1000):\n","    for i in range(len(xs)):\n","        # forward\n","        y = model(xs[i])\n","\n","        # calculate loss (mean square error)\n","        loss = (ys[i] - y) ** 2\n","        \n","        # backward (zero_grad + backward)\n","        loss.zero_grad()\n","        loss.backward()\n","        \n","        # update\n","        learning_rate = 0.01\n","        loss.update(learning_rate)\n","      \n","    if k % 100 == 99:\n","        print(f\"Epoch: {k}, loss: {loss.data:.4f}\")"],"metadata":{"id":"OuCTaTB8n5l0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670676408290,"user_tz":-180,"elapsed":1311,"user":{"displayName":"Djacon","userId":"03384204586246157387"}},"outputId":"30fe4185-eeba-431f-abc5-f8ee8aab723b"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 99, loss: 0.0231\n","Epoch: 199, loss: 0.0009\n","Epoch: 299, loss: 0.0002\n","Epoch: 399, loss: 0.0000\n","Epoch: 499, loss: 0.0000\n","Epoch: 599, loss: 0.0000\n","Epoch: 699, loss: 0.0000\n","Epoch: 799, loss: 0.0000\n","Epoch: 899, loss: 0.0000\n","Epoch: 999, loss: 0.0000\n","CPU times: user 1.21 s, sys: 10.2 ms, total: 1.22 s\n","Wall time: 1.25 s\n"]}]},{"cell_type":"code","source":["for i in range(len(xs)):\n","  print(round(model(xs[i]).data, 4), ys[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RrI9sthIwZBY","executionInfo":{"status":"ok","timestamp":1670676410223,"user_tz":-180,"elapsed":209,"user":{"displayName":"Djacon","userId":"03384204586246157387"}},"outputId":"97715f26-8778-4d66-98a3-f538e6264100"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["1.0001 1.0\n","-1.0 -1.0\n","-1.0 -1.0\n","0.9999 1.0\n"]}]},{"cell_type":"markdown","source":["# Домашнее задание"],"metadata":{"id":"n4maaWL5yg-f"}},{"cell_type":"markdown","source":["**Домашнее задание 1.** Доделать практику. Оформить код в три отдельных модуля `autograd`, `nn`, `train`"],"metadata":{"id":"2yyK39RYo084"}},{"cell_type":"markdown","source":["**Домашнее задание 2 (Опционально).** Создать свою функцию softmax, наследуемую от `torch.autograd.Function` и имплементировать forward и backward проход. Сравнить со стандартной функцией в Pytorch. \n","[Создание функций](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html) [Софтмакс](https://congyuzhou.medium.com/softmax-3408fb42d55a)"],"metadata":{"id":"FdzPyQ-hylKH"}},{"cell_type":"code","source":["values = torch.Tensor([[1., 2., 3.]])\n","values.requires_grad = True\n","\n","y_true = torch.Tensor([.3, .3, .3])\n","\n","y_pred = torch.nn.Softmax(dim=1)(values)\n","loss = (y_true - y_pred) ** 2\n","loss.sum().backward()\n","\n","values.data, values.grad, loss, y_true"],"metadata":{"id":"bGMpj9Pf61n2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670676576613,"user_tz":-180,"elapsed":217,"user":{"displayName":"Djacon","userId":"03384204586246157387"}},"outputId":"3cf8478d-0bdf-4477-ca88-9e7f75e9bc31"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1., 2., 3.]]),\n"," tensor([[-0.0757, -0.1301,  0.2058]]),\n"," tensor([[0.0441, 0.0031, 0.1334]], grad_fn=<PowBackward0>),\n"," tensor([0.3000, 0.3000, 0.3000]))"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["**Домашнее задание 3 (Опционально).** Добавить функцию софтмакс в собственную библиотеку автоматического дифференцирования. Сравнить с пунктом 2"],"metadata":{"id":"3VPpRO6H6SHF"}},{"cell_type":"markdown","source":["Создадим класс Sum для нахождения градиента сразу для нескольких значений"],"metadata":{"id":"_N1n_m6diyVo"}},{"cell_type":"code","source":["class Sum:\n","    def __call__(self, inputs): # Calculate multiple sum\n","        out = Value(sum(input.data for input in inputs), _children=tuple(inputs))\n","\n","        def _backward():\n","            for input in inputs:\n","                input.grad += out.grad\n","        out._backward = _backward\n","        out.grad_fn = '<AddBackward>'\n","        return out\n","\n","Sum = Sum()"],"metadata":{"id":"cq10fxZoTXTt","executionInfo":{"status":"ok","timestamp":1670676457358,"user_tz":-180,"elapsed":231,"user":{"displayName":"Djacon","userId":"03384204586246157387"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["Создадим собственную версию Softmax"],"metadata":{"id":"MpTJ7EN5jNa8"}},{"cell_type":"code","source":["class Softmax:\n","    def __call__(self, inputs): # Calculate softmax\n","        outs = [input.exp() for input in inputs]\n","        total = Sum(outs)\n","        outs = [out / total for out in outs]\n","        return outs\n","\n","    def __repr__(self):\n","        return f'<Softmax>'\n","\n","Softmax = Softmax()"],"metadata":{"id":"t4c-05O1COpO","executionInfo":{"status":"ok","timestamp":1670676540786,"user_tz":-180,"elapsed":2,"user":{"displayName":"Djacon","userId":"03384204586246157387"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["values = [Value(1.), Value(2.), Value(3.)]\n","\n","y_true = [.3, .3, .3]\n","\n","y_pred = Softmax(values)\n","loss = [(yt - yp) ** 2 for (yt, yp) in zip(y_true, y_pred)]\n","Sum(loss).backward()\n","\n","print(values, loss, y_true, sep='\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n710KEKf6FwU","executionInfo":{"status":"ok","timestamp":1670676677263,"user_tz":-180,"elapsed":230,"user":{"displayName":"Djacon","userId":"03384204586246157387"}},"outputId":"eff38887-8ae2-410c-88cd-bbbb6ca02bba"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["[Value(data=1.0, grad=-0.0757), Value(data=2.0, grad=-0.1301), Value(data=3.0, grad=0.2058)]\n","[Value(data=0.0441, grad=1, grad_fn=<PowBackward>), Value(data=0.0031, grad=1, grad_fn=<PowBackward>), Value(data=0.1334, grad=1, grad_fn=<PowBackward>)]\n","[0.3, 0.3, 0.3]\n"]}]},{"cell_type":"markdown","source":["Как видим, результат получился одинаковый!"],"metadata":{"id":"YG412aQgjyR_"}},{"cell_type":"markdown","source":["**Домашнее задание 4 (Опционально).** Добавить визуализацию обучения. Потом мы пройдем более подробно."],"metadata":{"id":"nRRgw0HNsr_a"}},{"cell_type":"markdown","source":["https://docs.wandb.ai/guides/integrations/pytorch"],"metadata":{"id":"W5AWW52REfn5"}},{"cell_type":"markdown","source":["https://docs.wandb.ai/ref/python/watch  "],"metadata":{"id":"ekFfy3cWVOIW"}},{"cell_type":"markdown","source":["https://docs.wandb.ai/guides/track/jupyter"],"metadata":{"id":"9G4SOp28ok0o"}},{"cell_type":"code","source":["!pip install wandb"],"metadata":{"id":"lumiR8oykL04"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wandb login"],"metadata":{"id":"Xw3c6P7BkP9b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import wandb\n","run = wandb.init(project=\"polynom_learning_\")"],"metadata":{"id":"udPv0ufwkxOv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["run.finish()"],"metadata":{"id":"Xtpc9MAUodNs"},"execution_count":null,"outputs":[]}]}